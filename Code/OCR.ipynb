{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OCR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqV3oHrg8mY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import os\n",
        "import torch\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from skimage import io\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "import PIL\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZPVvFlrIHMt",
        "colab_type": "code",
        "outputId": "3ecfeaa3-c425-4928-e093-8e7b7b67dbbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "%cd /content/\n",
        "if(os.path.isdir('/content/Lithuanian_OCR') == False):\n",
        "  !git clone https://github.com/PauliusMilmantas/Lithuanian_OCR"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'Lithuanian_OCR'...\n",
            "remote: Enumerating objects: 1198, done.\u001b[K\n",
            "remote: Counting objects: 100% (1198/1198), done.\u001b[K\n",
            "remote: Compressing objects: 100% (647/647), done.\u001b[K\n",
            "remote: Total 1198 (delta 541), reused 1139 (delta 483), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1198/1198), 1.84 MiB | 12.34 MiB/s, done.\n",
            "Resolving deltas: 100% (541/541), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dizldPQRNUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ORCDataset(Dataset):\n",
        "  def __init__(self, root):\n",
        "    self.root = root\n",
        "\n",
        "  def __len__(self):\n",
        "    lt = 0\n",
        "    classes = os.listdir(self.root)\n",
        "    for cl in classes:\n",
        "      lt += len(os.listdir(self.root + '/' + cl))\n",
        "\n",
        "    return lt\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "     idx = idx.tolist()\n",
        "\n",
        "    if(idx <= len(self)):\n",
        "      found_file = \"\"\n",
        "      found_type = \"\"\n",
        "\n",
        "      fldrs = os.listdir(self.root)\n",
        "      for fld in fldrs:\n",
        "        fls = os.listdir(self.root + '/' + fld + '/')\n",
        "        for fl in fls:\n",
        "          if(fl == str(idx) + \".jpg\"):\n",
        "            found_file = self.root + '/' + fld + '/' + fl         \n",
        "            found_type = fld\n",
        "\n",
        "      try:\n",
        "        img = io.imread(found_file)\n",
        "        # img = rgb2gray(img)\n",
        "\n",
        "        return {'image': img, 'class_name': found_type}\n",
        "      except:\n",
        "        if(found_file != \"\"):\n",
        "          print(\"Bad file: \" + found_file)\n",
        "        else:\n",
        "          print(\"File not found, idx = \" + str(idx))\n",
        "    else:\n",
        "      print()\n",
        "      raise Exception(\"Dataset index out of boundaries\")\n",
        "\n",
        "train_dataset = ORCDataset('/content/Lithuanian_OCR/Data/training')\n",
        "val_dataset = ORCDataset('/content/Lithuanian_OCR/Data/val')\n",
        "test_dataset = ORCDataset('/content/Lithuanian_OCR/Data/test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR2HO96n8pXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, hidden1_size, hidden2_size, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden2_size, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwTzb7K0_OSi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "2ff47db5-9600-426d-90b2-d3f3b09838c4"
      },
      "source": [
        "network = Net(595056, 130, 28, 20)\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=0.001, momentum=0.6)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "print(network)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=595056, out_features=130, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=130, out_features=28, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (fc3): Linear(in_features=28, out_features=20, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIOL9zLnKiEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = len(train_dataset),shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = len(val_dataset), shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = len(test_dataset), shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAi_TQWW-04_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "cc5cc4be-528b-4477-9f27-437f26262fb1"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "itr = dataiter.next()\n",
        "\n",
        "label = itr['class_name']\n",
        "img = itr['image']\n",
        "print(\"Class name: {}\".format(label[0]))\n",
        "\n",
        "fig = plt.figure(figsize = (5,5)) \n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class name: c2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f04d79c0240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAEvCAYAAAAtufaDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAWVklEQVR4nO3dW2xd5ZnG8ee1Y8eOnSM5kkDCQMJ5CqOITtVqxBxaaKUR7U1VVFWMVClVVaRW6sVUvSk3I6FRW2YqjSqlAy0d9aBWbadcwLQIITGVZhABUQikFIgShcTEISGJHXz2OxfeTE2wsx57r+ztD/4/KbK9/fpb39pr5/He2+/6VmSmAKBUHe2eAAA0gxADUDRCDEDRCDEARSPEABSNEANQtGWt3Nia1atz86aNrdzke0TUNlLdLTUR1XOre5sdHdW/e6enl27rkHGXvV1p1lXv63uhk+qPr7z8RmZuOP/2pkIsIm6X9K+SOiX9e2bee6H6zZs2au93vtPMJt+Xli2r73fN2NhYbWNJUnd3d2XN5OSkNdb09LRV19vbW1kzNjpV6zZdTqh3dnbWNpbk/ZJw97PuXzh1jvc3f//xw3PdvuiXkxHRKenfJH1c0nWS7oyI6xY7HgAsRjPvid0i6ZXMPJiZ45J+KumOeqYFAJ5mQmyrpCOzvn6tcRsAtMxF/+tkROyJiH0Rse/0mbMXe3MA3meaCbGjki6b9fW2xm3vkJl7M3N3Zu5es3pVE5sDgHdrJsSekrQzIq6IiG5Jn5H0UD3TAgDPov92n5mTEXG3pN9opsXigcx8obaZAYChqQakzHxY0sM1zQUAFqylHftYnDobBt3G2YmJCavOaWTt6emxxnIbMoeHhytrentWWmO594fboDo1Vd1kOz4+bo3lHoM6G2xdbiOuc3aFU3PBn2/qpwGgzQgxAEUjxAAUjRADUDRCDEDRCDEARSPEABSNEANQtBY3u4bIzYWbmKhuoHQbI/v7+6267m6vQfXNN9+srBkb8+a2cqXXoLp8efXKrkNDQ9ZYbhNoV1dXreM56lzR121OdVfhdR9vo6OjlTVu8+98SBQARSPEABSNEANQNEIMQNEIMQBFI8QAFI0QA1A0QgxA0QgxAEVr+fLUbucw/sTpsj916pQ1lrsE9NjYmFX30ksvVda89dZb1lhXX321Vbd1a/U1mldfutYay+koX0idc//W2YkveXMbGRmxxnLr3MeR09nvdv/Ph2diAIpGiAEoGiEGoGiEGICiEWIAikaIASgaIQagaIQYgKIRYgCK1vKOfSycs479JZdcYo3V0+Otnf/cc89ZdYODg5U1V155pTXW9u3brTqnQ31gYMAaq+419p0zUs6dO2eN9cYbb1h1x44dq6xxz+jo7a2+foHkXw9h/fr1lTWbN2+2xpoPz8QAFI0QA1A0QgxA0QgxAEUjxAAUjRADUDRCDEDRCDEARSPEABSNNfYLcMUVV1TWHDhwwBrLXd99165dVl1HR/Xvwaeeesoa65lnnrHq1q6tXj//sm1e9//U1JRV514n4MyZM5U17vULnGsrSN41B6699lprLKfDXqr3/7G7Xv98mgqxiDgkaUjSlKTJzNzd1GwAYIHqeCb215npneQFADXjPTEARWs2xFLSbyPi6YjYM1dBROyJiH0Rse+08X4BACxEsy8nP5KZRyNio6RHI+IPmfnE7ILM3CtpryRds2tXNrk9AHiHpp6JZebRxsdBSb+SdEsdkwIA16JDLCL6ImLl259L+pik/XVNDAAczbyc3CTpV41+kWWSfpyZ/1XLrADAtOgQy8yDkj6wsJ8KKfmD6EI9+b/VzaLuEtCrVq2y6pymTUnafnl1I+4H/vxma6wTJ05Ydc7S2UeOHLHGWrNmjVW3YcMGq+6qq66qrOnu7rbGchuTnaWz3eW13cbTyclJq258fLyyxm04ng+JAqBohBiAohFiAIpGiAEoGiEGoGiEGICiEWIAikaIASgaIQagaC1fnhoLd80111TWjIyMWGMNDAxYdW5XudMJfuzYMWusTG+Rkw996EOVNW63u7vstHv/Oss29/T0WGO5XfYTExOVNU7nvDuWJHV2dlp1vb29lTXOEucX/PmmfhoA2owQA1A0QgxA0QgxAEUjxAAUjRADUDRCDEDRCDEARSPEABStpR37OT2tsbGxyrqVK1dWj2V2d7trxbvjOWvU190tPjw8XFnjdnf39fVZde794dS5Heput7izFn9/f7811rlz56w65zEpeR37o6Oj1ljuMXD2we2wd+vcublr8TeDZ2IAikaIASgaIQagaIQYgKIRYgCKRogBKBohBqBohBiAorW02bWjs8NqtnSaO90lbdetW2fVuU15Z8+eraxxGh4lr3HWHc9tPnSXKXbrHO5S18uXL6+tzm2cdefm3h/T09OVNe7jwz2m7j68V/FMDEDRCDEARSPEABSNEANQNEIMQNEIMQBFI8QAFI0QA1A0QgxA0Vrasa8MZVZ3K3d3Vy9nPDU1ZW3y3LkRq85dlrevr76ls8fGvK7yU6dOVdY4neILqfPvj+ozMDo6vIeZOTXr7AqzKd5edtpdStw5U6C3t9cay32MO0uTu2cwlKjymVhEPBARgxGxf9Zt6yLi0Yh4ufFx7cWdJgDMzXk5+QNJt59329ckPZaZOyU91vgaAFquMsQy8wlJ57+euUPSg43PH5T0yZrnBQCWxb6xvykzBxqfvy5pU03zAYAFafqvkznzLva872RHxJ6I2BcR+948c7rZzQHAOyw2xI5HxBZJanwcnK8wM/dm5u7M3L129ZpFbg4A5rbYEHtI0l2Nz++S9Ot6pgMAC+O0WPxE0v9IujoiXouIz0u6V9JHI+JlSX/X+BoAWq6yCzEz75znW39b81wAYMFa2rE/ndPWWuXLllVPy11j310b3e2OdtZHd9bhl6STJ09adSMj1WcduOv1uwYGBqqLJB05cqSyxj1LYPPmzVbdli1bKmu2bbvUGsvt2F+xYoVV5+yrey2BoaEhq87p2HePgfv/aikpb8YAMAshBqBohBiAohFiAIpGiAEoGiEGoGiEGICiEWIAikaIAShaSzv2Q2F1vDvrmXd3d1vbdNaAl/zu+YMHD1bWuJ3W7tx27dpVWeOuoe7c/5J0+eWXW3W33XZbZc3w8LA11v79+6uLJP3+97+vrJmc9M7UcM/ouPRS7wwAp+Pd7Yp3zyJxlNiJ73rv7hmA9wVCDEDRCDEARSPEABSNEANQNEIMQNEIMQBFI8QAFK2lza6ptJbJdZYCdps7Dx8+bNUdO3bMqpucnKyscRsj3eWYnQbVNWu8y+E5SxlL3n5K0qlT518c/t3cBtvrr7/eqrvxxhsra37zm0essZz5S9LatWutOud+cxts3fvN4Sz5LvnHfSnhmRiAohFiAIpGiAEoGiEGoGiEGICiEWIAikaIASgaIQagaIQYgKK1tGNfKU1NZWVZd3dnZc3p0+5y0oesuszqeUnSDTfcUFnjLu3sbtNZ3rmzs/o+m+F2gXu/3yKqt+t2no+Pe93iTlf5qlWrrLGOHDli1dW5vLPbFe922TvLWNfZ/b/U8EwMQNEIMQBFI8QAFI0QA1A0QgxA0QgxAEUjxAAUjRADUDRCDEDRWtuxH2F1IR89erSyZmBgwNpkX1+fVed22W/durWyxl1D/eRJ76yD5cuXW3WOOjvP3fHcbbpnHTiPoR07dlhjOY81yX8cOdeQcM/UqLNj/72s8tEVEQ9ExGBE7J912z0RcTQinm38+8TFnSYAzM35FfkDSbfPcft9mXlT49/D9U4LADyVIZaZT0jyrmsFAC3WzBskd0fEc42Xm/NelC8i9kTEvojYd+bM6SY2BwDvttgQ+66kKyXdJGlA0rfmK8zMvZm5OzN3r17tXeAVAFyLCrHMPJ6ZU5k5Lel7km6pd1oA4FlUiEXElllffkrS/vlqAeBiqmxEiYifSLpV0vqIeE3SNyTdGhE3SUpJhyR94SLOEQDmVRlimXnnHDffv5iNhaQOY3nk0bdGKmteffkVa5u33XabVdff32/VhdGnODYyao3V17vCqpuYmKgu6vQaKGd+7xhVU9VNm5K37LFzn0nmfkoaGal+fBz4wwvWWDfceJ1V9/rxY1ad0xTrNrtOTHr3h3UMlvDy1M3OjdOOABSNEANQNEIMQNEIMQBFI8QAFI0QA1A0QgxA0QgxAEUjxAAUrbXLU8vrzt2wYUNljbMMsOQv3evWnThxorJm5cqV1ljd3d1WnbOv7pLYTrf7QqxYUX3WQVdXlzWW27E/NjZWWeMeA2f+ktTT02PVOUuJux377mPcGc8dy61bSngmBqBohBiAohFiAIpGiAEoGiEGoGiEGICiEWIAikaIASgaIQagaC3t2I+QOjurO/ZPnqzuij992rso+Zo1q6y6tWvnvf7vedutvgDwypXV66xL0quvvmrVrV+/vrLGPeOgo+ZfWxMT1d3zw8NnrbEGBwetOuesiW3bL7XGWrXKe3y4Zx04x6HOTny3zh2rHZqdG8/EABSNEANQNEIMQNEIMQBFI8QAFI0QA1A0QgxA0QgxAEUjxAAUraUd+5lpraO+bt26yhp3DfWDBw9adVdddZVV56zvPjQ0ZI21bds2q84Zz12v311T3u0qHx0draxxzybo7e216rZs2VJZc+mlXsd+Z2enVed2lTuPD//sCu85hlvncK6BsdTwTAxA0QgxAEUjxAAUjRADUDRCDEDRCDEARSPEABSNEANQtJY2u05nWs2A27dvr6xxGmIl6fHHH7fq3GWKd+7cWVlz+PBha6yNGzdadU7jqdNELPmNlm5zpzNeT0+PNdaaNWusOme8M8Pe8uXtWLbZbbBdtsz77+k0u9Z5PJeayr2PiMsi4vGIeDEiXoiILzduXxcRj0bEy42P3iL1AFAj5+XkpKSvZuZ1kv5S0pci4jpJX5P0WGbulPRY42sAaKnKEMvMgcx8pvH5kKQDkrZKukPSg42yByV98mJNEgDms6A39iNih6SbJT0paVNmDjS+9bqkTbXODAAMdohFRL+kX0j6Sma+40KCOfOu4ZzvHEbEnojYFxH7zpw509RkAeB8VohFRJdmAuxHmfnLxs3HI2JL4/tbJM155dPM3JuZuzNz9+rVq+uYMwD8P+evkyHpfkkHMvPbs771kKS7Gp/fJenX9U8PAC7MaUT5sKTPSXo+Ip5t3PZ1SfdK+llEfF7SYUmfvjhTBID5VYZYZv5O0nzLPf5tvdMBgIVpacd+RKirq6uy7uTJk5U17nLShw4dsuoeeeQRq855X88540CSBgfnfBvxXfr6+ipr6u7Yd7vFnTp3bmfPnq0uknTqVHU3/umh6seQ5N23C6lzlgl3O/bdOkeJnfguzp0EUDRCDEDRCDEARSPEABSNEANQNEIMQNEIMQBFI8QAFI0QA1C01nbsS+owmpDHJ0Yra6697mprm8PnvC7wn//851bdD//jB5U1n/3sZ62xVqxYYdWNjY9U1kxOTlpj2aaq1/WXvE7wc+fOWWO5dda+dtS7dr6zjr2r7vXunfGc6zRIM2fVtFqz1zngmRiAohFiAIpGiAEoGiEGoGiEGICiEWIAikaIASgaIQagaC1tdp2entbISHXj5iWXXFJZc+LECWubH/zgB62666+/3qr7/ve/X1lz3333WWPt2LHDqnOW4m62YfB8bnOk03jqLrO8YcMGq27jxo2VNct7q5eJlqSenh6rzllWXfIaVN3lul3O/eseA/e4LyU8EwNQNEIMQNEIMQBFI8QAFI0QA1A0QgxA0QgxAEUjxAAUjRADULSWdux3dHRYSzI7yxQvX77c2ubw8LBV53Zuf/GLX6ysefPNN62x3LMOhoaGKmvc+ff29lp17nj9/f211Ej+MXWMjL1l1blnOoyPjzcznXeoc6lrqf6zNVqt2SWxeSYGoGiEGICiEWIAikaIASgaIQagaIQYgKIRYgCKRogBKBohBqBoLe3YT2Vta7IvW1bv1N21xZ25OWclSP6a8lu3bq2sce8Pt1vcvT+cbmu3o3x0dNSqc9ax71jWXBc4ylH5iI6IyyLi8Yh4MSJeiIgvN26/JyKORsSzjX+fuPjTBYB3cn59T0r6amY+ExErJT0dEY82vndfZn7z4k0PAC6sMsQyc0DSQOPzoYg4IKn69Q0AtMCC3tiPiB2Sbpb0ZOOmuyPiuYh4ICLW1jw3AKhkh1hE9Ev6haSvZOZZSd+VdKWkmzTzTO1b8/zcnojYFxH7zpw5W8OUAeBPrBCLiC7NBNiPMvOXkpSZxzNzKjOnJX1P0i1z/Wxm7s3M3Zm5e/XqVXXNGwAkeX+dDEn3SzqQmd+edfuWWWWfkrS//ukBwIU5f538sKTPSXo+Ip5t3PZ1SXdGxE2SUtIhSV+4KDMEgAtw/jr5O0lzdQ4+XP90AGBhWtqxr5Qyq7utI6rfqnM65yWvu1uSJie9NdQzq7fb3e3drV1dK606Z1/d+8PtnnfOrJC8zv4Ib5vuWQJTUxOVNR3Luq2xUD7OnQRQNEIMQNEIMQBFI8QAFI0QA1A0QgxA0QgxAEUjxAAUrbXNruE1Zda55LFb5zbFWksjm0tAuw2qzrLN7vLUzn0r1bs8dd1LZ3d1dVl1eH/gmRiAohFiAIpGiAEoGiEGoGiEGICiEWIAikaIASgaIQagaIQYgKKF29Fey8YiTkg6fN7N6yW90bJJ1K/0+Uvl70Pp85fK34dWzH97Zm44/8aWhthcImJfZu5u6ySaUPr8pfL3ofT5S+XvQzvnz8tJAEUjxAAUbSmE2N52T6BJpc9fKn8fSp+/VP4+tG3+bX9PDACasRSeiQHAorUtxCLi9oh4KSJeiYivtWsezYiIQxHxfEQ8GxH72j0fR0Q8EBGDEbF/1m3rIuLRiHi58XFtO+d4IfPM/56IONo4Ds9GxCfaOccLiYjLIuLxiHgxIl6IiC83bi/pGMy3D205Dm15ORkRnZL+KOmjkl6T9JSkOzPzxZZPpgkRcUjS7swspr8nIv5K0rCkH2bmDY3b/lnSqcy8t/ELZW1m/mM75zmfeeZ/j6ThzPxmO+fmiIgtkrZk5jMRsVLS05I+KekfVM4xmG8fPq02HId2PRO7RdIrmXkwM8cl/VTSHW2ay/tKZj4h6dR5N98h6cHG5w9q5gG5JM0z/2Jk5kBmPtP4fEjSAUlbVdYxmG8f2qJdIbZV0pFZX7+mNt4JTUhJv42IpyNiT7sn04RNmTnQ+Px1SZvaOZlFujsinmu83FyyL8Vmi4gdkm6W9KQKPQbn7YPUhuPAG/vN+Uhm/oWkj0v6UuOlTtFy5v2F0v5k/V1JV0q6SdKApG+1dzrVIqJf0i8kfSUzz87+XinHYI59aMtxaFeIHZV02ayvtzVuK0pmHm18HJT0K828TC7R8cb7HG+/3zHY5vksSGYez8ypzJyW9D0t8eMQEV2a+c//o8z8ZePmoo7BXPvQruPQrhB7StLOiLgiIrolfUbSQ22ay6JERF/jTU1FRJ+kj0naf+GfWrIeknRX4/O7JP26jXNZsLf/8zd8Skv4OMTMNe7ul3QgM78961vFHIP59qFdx6Ftza6NP7/+i6ROSQ9k5j+1ZSKLFBF/pplnX9LM9Tt/XMI+RMRPJN2qmVUHjkv6hqT/lPQzSZdrZpWRT2fmknzzfJ7536qZlzAp6ZCkL8x6f2lJiYiPSPpvSc9Levvinl/XzHtKpRyD+fbhTrXhONCxD6BovLEPoGiEGICiEWIAikaIASgaIQagaIQYgKIRYgCKRogBKNr/AegFsSi01wWJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMbDr5k1BOPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(10 + 1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCwc6dI0BfdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNsrIgqgbT15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def name_to_int(data):\n",
        "\n",
        "  switcher = {\n",
        "      'a': 1,\n",
        "      'a2': 2,\n",
        "      'b': 3,\n",
        "      'c': 4,\n",
        "      'c2': 5,\n",
        "      'd': 6,\n",
        "      'e': 7,\n",
        "      'e2': 8,\n",
        "      'e3': 9,\n",
        "      'f': 10,\n",
        "      'g': 11,\n",
        "      'h': 12,\n",
        "      'i': 13,\n",
        "      'i2': 14,\n",
        "      'j': 15,\n",
        "      'k': 16,\n",
        "      'l': 17,\n",
        "      'm': 18,\n",
        "      'n': 19,\n",
        "      'y': 20\n",
        "  }\n",
        "\n",
        "  new_data = []\n",
        "\n",
        "  for dt in data:\n",
        "    new_data.append(switcher.get(dt))\n",
        "\n",
        "  return new_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE8MNszDBZ_e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "f73a90bf-4dd4-49af-b981-3c3c1adf2986"
      },
      "source": [
        "def train(train_loader):\n",
        "  network.eval()\n",
        "  for data in train_loader:\n",
        "    images = data['image']\n",
        "    labels = torch.from_numpy(np.array(name_to_int(data['class_name'])))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = network(images.flatten().float())\n",
        "\n",
        "    loss = criterion(outputs, images.view(-1, 1)) \n",
        "\n",
        "\n",
        "\n",
        "    print(labels)\n",
        "    print(outputs)\n",
        "\n",
        "    print(labels.size())\n",
        "    print(outputs.size())\n",
        "\n",
        "train(train_loader)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 2,  5,  9, 14, 19,  8, 17, 14,  4, 11, 11, 13,  6, 18,  7,  5, 14, 18,\n",
            "         2, 10, 15, 12, 17, 17,  7, 17, 15, 18,  4, 13,  7, 10, 17, 15,  1, 14,\n",
            "        14,  1, 17, 16,  2, 11, 15, 19, 16,  9, 17,  7,  1, 13, 12,  3, 14, 12,\n",
            "        12, 13, 15, 12, 13,  9,  1, 19, 12, 11, 19, 20,  2,  2,  6,  3, 16,  1,\n",
            "        13,  2, 19, 10, 11,  1,  4, 17,  3, 10,  1, 13, 11, 20,  3, 16, 14,  5,\n",
            "        15,  5,  8, 16,  6,  7, 18,  2,  7,  4,  2, 19, 18, 11, 19, 15, 12, 13,\n",
            "        15, 10, 15, 17,  4, 10, 18,  3, 12, 10,  7, 15,  8, 20,  1, 13, 16, 18,\n",
            "         8, 18, 12,  5, 13,  1, 13,  8, 20, 19, 18,  4,  8,  5,  2,  4,  2,  9,\n",
            "        12, 19, 16, 12,  6,  6,  1, 12, 10, 20,  9, 10,  6,  1,  9, 10, 15,  4,\n",
            "        16,  6,  5, 17, 16,  5, 14, 17,  8, 10, 14,  3, 15,  9,  1,  7,  2,  7,\n",
            "        19,  2,  2,  3,  7, 20,  9, 13, 15, 20,  1, 20,  9, 10, 11, 17,  6,  1,\n",
            "         3, 19,  6, 10,  8,  1, 16,  2,  3,  5,  6, 12, 13,  1,  6, 14,  8, 19,\n",
            "        17, 16, 19, 18,  1,  5,  3, 20, 13, 17, 14, 15,  2,  1, 14, 11, 18,  4,\n",
            "         9, 13, 11,  2, 20,  2,  2, 20, 10,  5, 11, 14,  8,  1,  4,  9,  7, 12,\n",
            "         8])\n",
            "tensor([ 13.3378,  20.2990,  25.3360,  -4.1486,  -6.6098,   0.2526,  25.4795,\n",
            "          4.3241, -28.1605,  -8.1837,  -2.0775,  11.7416,  -0.6800,  26.7588,\n",
            "         -3.9208,   2.4867, -13.1660, -20.9474, -27.2188, -19.2198],\n",
            "       grad_fn=<AddBackward0>)\n",
            "torch.Size([253])\n",
            "torch.Size([20])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([595056, 1])) that is different to the input size (torch.Size([20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}